import agent_security.autoguard_code.generated_security.security_anomaly_zero as security_anomaly_zero
import agent_security.autoguard_code.generated_security.security_signature_zero as security_signature_zero
import agent_security.autoguard_code.generated_security.security_anomaly_few as security_anomaly_few
import agent_security.autoguard_code.generated_security.security_signature_few as security_signature_few

import time
from google import generativeai
import pandas as pd
import numpy as np
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report
import matplotlib.pyplot as plt

''' Ask the user on which files to download the test datasets from '''
prompt_injection_filename = input("Prompt Injection File: ")
genuine_input_filename = input("Genuine Input File: ")
test_size = input("How many prompts for each? ")

''' Download the test datasets '''
prompt_injection_file = open(f"/home/yuxuan/Documents/summer_internship/gensec-liu-yuxuan/agent_security/data/input_data/{prompt_injection_filename}.txt", 'r')
lines = prompt_injection_file.read()
prompt_injections = lines.split("\n~~~~~~~~~~~~~~~~~~~\n")[:int(test_size)]

genuine_input_file = open(f"/home/yuxuan/Documents/summer_internship/gensec-liu-yuxuan/agent_security/data/input_data/{genuine_input_filename}.txt", 'r')
lines = genuine_input_file.read()
genuine_inputs = lines.split("\n~~~~~~~~~~~~~~~~~~~\n")[:int(test_size)]
print("Loading complete ...")

total_inputs = prompt_injections + genuine_inputs

''' Set the Generative AI model '''
model = generativeai.GenerativeModel('models/gemini-1.5-pro-latest')

def evaluate_runs(datatype, security_guard_node, predicted):
    # Returns observations and results of the security guard's processing of the test datasets

    # Parameters:
    #       datatype (Any -> str): The type of the test dataset (either prompt injection or genuine input)
    #       security_guard_node (Any -> function): The function that is called to evaluate the inputs of the test datasets as either prompt injection or genuine input
    #       predicted (Any -> list): The list that keeps track of the predicted type generated by the security guard node for each input; used to create the confusion matrix    
    
    # Returns:
    #       accurate (int): The number of accurate predictions
    #       inaccurate (int): The number of inaccurate predictions
    #       accurate_record (dict): A python dictionary that records each accurately predicted input and the security guard's reasoning for its prediction
    #       inaccurate_record (dict): A python dictionary that records each inaccurately predicted input and the security guard's reasoning for its prediction
    #       errors (dict): A python dictionary that records the inputs that caused errors in the code with explainations to why the errors occured
    #       token_usages (list): The python list that records the amount of tokens used by the security guard LLM when processing each input
    #       cost (list): The python list that records the cost for the security guard LLM to process each input
    #       predicted (Any -> list): The updated list for tracking the predicted outcomes of the security guard

    ''' Setting variables '''
    accurate = 0
    inaccurate = 0
    errors = {}
    inaccurate_record = {}
    accurate_record = {}
    token_usages = []
    cost = []

    ''' Determine which test dataset to use '''
    if datatype == "prompt_injections":
        test_set = prompt_injections
    elif datatype == "genuine_inputs":
        test_set = genuine_inputs
    
    ''' Run each input in the test dataset'''
    for i in range(len(test_set)):
        result = ""
        print(f"Testing {i}")
        print(test_set[i])

        ''' Manage the errors gracefully '''
        try:
            detection_prompt = ""
            result = ""
            runs = 0

            ''' Run the input through the security guard node, repeat until a result is generated'''
            while result == "":
                print(f"Run: {runs}")
                detection_prompt, user_prompt, result = security_guard_node(test_set[i])
                runs += 1

            ''' Calculate token usage of the input '''
            input_tokens = int(str(model.count_tokens(detection_prompt)).split(' ')[1]) - int(str(model.count_tokens("{user_input}")).split(' ')[1]) + int(str(model.count_tokens(user_prompt)).split(' ')[1])
            token_usages += [input_tokens]

            ''' Calculate the cost of processing the input '''
            if input_tokens <= 128000:
                input_cost = input_tokens * 3.50 / 1000000
            else:
                input_cost = input_tokens * 7.00 / 1000000
            cost += [input_cost]

        except Exception as e:
            print(f"Error found: {e}")
            errors[i] = e
            token_usages += ["N\A"]
            cost += ["N\A"]
        
        ''' Determine if the result indicates prompt injection'''
        if "Inappropriate Input" in result:
            detected_injection = True
            print("Injection Detected")
        elif "Prompt Injection" in result:
            detected_injection = True
            print("Injection Detected")
        else:
            detected_injection = False

        ''' Determine the accuracy of security guard's prediction based upon the type of the test dataset'''
        if datatype == "prompt_injections":
            if detected_injection == True:
                accurate += 1
                accurate_record[i] = result
                predicted += ["Prompt Injection"]
            else:
                inaccurate += 1
                predicted += ["Not Prompt Injection"]
                inaccurate_record[i] = result
        elif datatype == "genuine_inputs":
            if detected_injection == True:
                inaccurate += 1
                predicted += ["Prompt Injection"]
                inaccurate_record[i] = result
            else:
                accurate += 1
                predicted += ["Not Prompt Injection"]
                accurate_record[i] = result

    return accurate, inaccurate, accurate_record, inaccurate_record, errors, token_usages, cost, predicted

def create_csv(predicted, actual, total_inputs, code, total_cost, total_token_usages):
    # Creates a csv file that stores information on the type, token usage, cost, and prediction accuracy of each input

    # Parameters:
    #       predicted (Any -> list): The list that keeps track of the predicted type generated by the security guard node for each input; used to create the confusion matrix    
    #       actual (Any -> list): The list that keeps track of the actual type of each input; used to create the confusion matrix
    #       total_inputs (Any -> list): The list of all inputs - combining the test datasets of prompt injections and genuine inputs
    #       code (Any -> str): The approach used by the security guard node
    #       total_cost (Any -> list): The list that records the cost for the security guard to process each input in both of the test datasets
    #       total_token_usages (Any -> list): The list that records the token usage for each input in both of the test datasets

    # Returns:
    #       None

    ''' Set a list for recording all the needed context '''
    data = []

    ''' Retrieve data for each input in the two test datasets '''
    for i in range(len(total_inputs)):
        input = total_inputs[i]
        print(f"Testing {i}: {input}")
        if actual[i] == "Prompt Injection":
            is_prompt_injection = True
            if predicted[i] == "Prompt Injection":
                correctly_classified = True
            else:
                correctly_classified = False
        elif actual[i] == "Not Prompt Injection":
            is_prompt_injection = False
            if predicted[i] == "Prompt Injection":
                correctly_classified = False
            else:
                correctly_classified = True
        token_usage = total_token_usages[i]
        cost = total_cost[i]

        data.append([is_prompt_injection, input, token_usage, cost, correctly_classified])

    ''' Convert list into a pandas dataframe '''
    df = pd.DataFrame(data, columns = [f'Prompt Injection', 'Input', 'Token Usage', 'Cost', 'Correctly Classified'])
    print(df)

    ''' Convert pandas dataframe into csv file '''
    df.to_csv(f'/home/yuxuan/Documents/summer_internship/gensec-liu-yuxuan/agent_security/data/output_data/{code}.csv')

    return

def calculate_stats(true_positives, true_negatives, false_positives, false_negatives, total_prompts):
    # Calculate the statistics for each security guard approach using the raw data - true positives, true negatives, false positives, false negatives, and total inputs

    # Parameters:
    #       true_positives (Any -> int): Number of true positives (the security guard accurately predicted prompt injections)
    #       true_negatives (Any -> int): Number of true negatives (the security guard accurately predicted genuine inputs)
    #       false_positives (Any -> int): Number of false positives (the security guard predicted genuine inputs as prompt injections)
    #       false_negatives (Any -> int): Number of false negatives (the security guard predicted prompt injections as genuine inputs)
    #       total_prompts (Any -> int): Total number of inputs - both prompt injections and genuine inputs

    # Returns:
    #       accuracy (Any -> float): Accuracy of security guard
    #       precision (Any -> float): Precision of security guard
    #       recall (Any -> float): Recall of security guard
    #       specificity (Any -> float): Specificity of security guard
    #       f1_score (Any -> float): F1-score of security guard
    #       type_1 (Any -> float): Type 1 error of security guard
    #       type_2 (Any -> float): Type 2 error of security guard
    
    ''' Accuracy: Performance of the model - total correct in all instances '''
    accuracy = (true_positives + true_negatives) / total_prompts

    ''' Precision: How accurate the positive (prompt injection) results are - true positives out of all positive predictions '''
    precision = true_positives / (true_positives + false_positives)

    ''' Recall: Effectiveness to identify relevant instances in a dataset - true positives out of all real positives '''
    recall = true_positives / (true_positives + false_negatives)

    ''' Specificity: Effectiveness to identify negative instances - true negatives out of all real negatives '''
    specificity = true_negatives / (true_negatives + false_positives)

    ''' F1-score: overall performance used when a balance needs to be reached between precision and recall - harmonic mean of preciison and recall '''
    f1_score = 2*precision*recall / (precision + recall)

    ''' Type 1 error is when the model identifies a real negative as positive, affecting the precision '''
    type_1 = false_positives / (true_negatives + false_positives)
    
    ''' Type 2 error is when the model identifies a real positive as negative, affecting the recall '''
    type_2 = false_negatives / (true_positives + false_negatives)

    ''' Side note: Use precision if it is necessary to minimize false positive and recall if it is necessary to minimize false negatives '''

    ''' Print out the data '''
    print(f"Accuracy: {accuracy}\nPrecision: {precision}\nRecall: {recall}\nSpecificity: {specificity}\nF1-score: {f1_score}\nType 1 error: {type_1}\nType 2 error: {type_2}")
    
    return accuracy, precision, recall, specificity, f1_score, type_1, type_2

def __main__(code):
    # The main function that runs and evaluates the security guard approach and create txt documents of the processed results

    # Parameters:
    #       code (Any -> str): the approach that the security guard uses

    # Returns:
    #       None

    ''' Initialize the security guard node based on the approach to use '''
    print(f"Running {code} ...")
    if code == "anomaly_zero":
        security_guard_node = security_anomaly_zero.security_guard_node
    elif code == "signature_zero":
        security_guard_node = security_signature_zero.security_guard_node
    elif code == "anomaly_few":
        security_guard_node = security_anomaly_few.security_guard_node
    elif code == "signature_few":
        security_guard_node = security_signature_few.security_guard_node

    ''' Set the lists for actual type and predicted type (used to create confusion matrix) '''
    actual = len(prompt_injections) * ["Prompt Injection"] + len(genuine_inputs) * ["Not Prompt Injection"]
    predicted = []

    ''' Obtain the raw data using the evaluate_runs function '''
    true_positives, false_negatives, tp_record, fn_record, prompt_injection_errors, token_usages_prompt_injections, cost_prompt_injections, predicted = evaluate_runs("prompt_injections", security_guard_node, predicted)
    true_negatives, false_positives, tn_record, fp_record, genuine_input_errors, token_usages_genuine_inputs, cost_genuine_inputs, predicted = evaluate_runs("genuine_inputs", security_guard_node, predicted)

    ''' Create the csv file using the create_csv function '''
    total_cost = cost_prompt_injections + cost_genuine_inputs
    total_token_usages = token_usages_prompt_injections + token_usages_genuine_inputs
    create_csv(predicted, actual, total_inputs, code, total_cost, total_token_usages)
    
    ''' Print out the data '''
    total_prompts = len(prompt_injections) + len(genuine_inputs)
    print(f"Total: {total_prompts}, Prompt Injections: {len(prompt_injections)}, Genuine Inputs: {len(genuine_inputs)} \nTrue Positives: {true_positives}, True Negatives: {true_negatives}, False Positives: {false_positives}, False Negatives: {false_negatives}")
    print(f"Actual: {actual}")
    print(f"Predicted: {predicted}")

    ''' Obtain the statistics using the calculate_stats function '''
    accuracy, precision, recall, specificity, f1_score, type_1, type_2 = calculate_stats(true_positives, true_negatives, false_positives, false_negatives, total_prompts)
    class_report = classification_report(actual, predicted)
    print(class_report)

    ''' Document the actual and predicted lists to a txt file that will be used to create a confusion matrix '''
    f = open(f'/home/yuxuan/Documents/summer_internship/gensec-liu-yuxuan/agent_security/data/output_data/{code}_cm.txt', 'w')
    f.write("Actual Result\n~~~~~\n")
    for i in actual:
        f.write(f"{i}, ")
    f.write("\n~~~~~~\n")
    f.write("Predicted Result\n~~~~~\n")
    for i in predicted:
        f.write(f"{i}, ")
    f.close()

    ''' Document the records of the security guard's predictions and reasonings into txt files '''
    f = open(f'/home/yuxuan/Documents/summer_internship/gensec-liu-yuxuan/agent_security/data/output_data/{code}_false_negative_records.txt', 'w')
    for i in fn_record:
        f.write(f"Prompt: {prompt_injections[i]}\nResult: {fn_record[i]}\n~~~~~~~~~~~~~~~~~~~~~~~~\n")
    f.close()
    f = open(f'/home/yuxuan/Documents/summer_internship/gensec-liu-yuxuan/agent_security/data/output_data/{code}_false_positive_records.txt', 'w')
    for i in fp_record:
        f.write(f"Prompt: {genuine_inputs[i]}\nResult: {fp_record[i]}\n~~~~~~~~~~~~~~~~~~~~~~~~\n")
    f.close()
    f = open(f'/home/yuxuan/Documents/summer_internship/gensec-liu-yuxuan/agent_security/data/output_data/{code}_true_positive_records.txt', 'w')
    for i in tp_record:
        f.write(f"Prompt: {prompt_injections[i]}\nResult: {tp_record[i]}\n~~~~~~~~~~~~~~~~~~~~~~~~\n")
    f.close()
    f = open(f'/home/yuxuan/Documents/summer_internship/gensec-liu-yuxuan/agent_security/data/output_data/{code}_true_negative_records.txt', 'w')
    for i in tn_record:
        f.write(f"Prompt: {genuine_inputs[i]}\nResult: {tn_record[i]}\n~~~~~~~~~~~~~~~~~~~~~~~~\n")
    f.close()

    ''' Document the stats into txt file '''
    f = open(f'/home/yuxuan/Documents/summer_internship/gensec-liu-yuxuan/agent_security/data/output_data/{code}_stats.txt', 'w')
    f.write(f"Total {total_prompts}\nPromptInjections {len(prompt_injections)}\nGenuineInputs {len(genuine_inputs)}\nTruePositives {true_positives}\nTrueNegatives {true_negatives}\nFalsePositives {false_positives}\nFalseNegatives {false_negatives}\nAccuracy {accuracy}\nPrecision {precision}\nRecall {recall}\nSpecificity {specificity}\nF1-score {f1_score}\nType1 {type_1}\nType2 {type_2}")
    f.write(f"\nClassificiation Report: {class_report}")
    f.close()

    return

__main__("anomaly_zero")
__main__("signature_zero")
__main__("anomaly_few")
__main__("signature_few")